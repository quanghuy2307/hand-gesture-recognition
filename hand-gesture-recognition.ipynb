{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj_pmuLTIeI5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Cài đặt các thư viện cần dùng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMh8TbnPRK_u",
    "outputId": "2d7df28b-4485-4ffd-944b-57082c751e97",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG0L-P1CJY1H",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Chuẩn bị dữ liệu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd9Txn4gIpPv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hàm load ảnh từ thư mục train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDxUZ10fqm3n",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_images(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx, label in enumerate(uniq_labels):\n",
    "        print(label,\" is ready to load\")\n",
    "        for file in os.listdir(directory + \"/\" + label):\n",
    "            filepath = directory + \"/\" + label + \"/\" + file\n",
    "            image = cv2.resize(cv2.imread(filepath), (64, 64))\n",
    "            images.append(image)\n",
    "            labels.append(idx)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDUNixxFIydK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load ảnh train từ thư mục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cVHKiDY5qm7Z",
    "outputId": "f239c9d5-fc90-4054-8182-a8619eb2e527",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circle  is ready to load\n",
      "Fist  is ready to load\n",
      "Nothing  is ready to load\n",
      "Ok  is ready to load\n",
      "Palm  is ready to load\n",
      "Peace  is ready to load\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ACER\\Desktop\\Nhom_5\\hand-gesture-recognition.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=2'>3</a>\u001b[0m uniq_labels \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=3'>4</a>\u001b[0m images, labels \u001b[39m=\u001b[39m load_images(directory \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mData has been loaded\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ACER\\Desktop\\Nhom_5\\hand-gesture-recognition.ipynb Cell 7\u001b[0m in \u001b[0;36mload_images\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(directory \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m label):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=6'>7</a>\u001b[0m     filepath \u001b[39m=\u001b[39m directory \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m label \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m file\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=7'>8</a>\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(cv2\u001b[39m.\u001b[39;49mimread(filepath), (\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=8'>9</a>\u001b[0m     images\u001b[39m.\u001b[39mappend(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER/Desktop/Nhom_5/hand-gesture-recognition.ipynb#ch0000006?line=9'>10</a>\u001b[0m     labels\u001b[39m.\u001b[39mappend(idx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "uniq_labels = sorted(os.listdir(\"train\"))\n",
    "images, labels = load_images(directory = \"train\")\n",
    "print(\"Data has been loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMCD7ocdqnEV",
    "outputId": "3cc60f5f-2667-4913-ffe9-7443cccdc54c",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of symbols:  9\n",
      "Number of training images:  21600\n",
      "Number of testing images:  5400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, stratify = labels)\n",
    "\n",
    "n = len(uniq_labels)\n",
    "train_n = len(X_train)\n",
    "test_n = len(X_test)\n",
    "\n",
    "print(\"Total number of symbols: \", n)\n",
    "print(\"Number of training images: \", train_n)\n",
    "print(\"Number of testing images: \", test_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JXa4Cwoqm-O",
    "outputId": "f2067615-f10b-49e0-80a8-7e98eaf0c1ae",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train_in = y_train.argsort()\n",
    "y_train = y_train[y_train_in]\n",
    "X_train = X_train[y_train_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtPxEkKaqm2K",
    "outputId": "c2ec392e-5986-48a2-c814-a236659bb7d1",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test_in = y_test.argsort()\n",
    "y_test = y_test[y_test_in]\n",
    "X_test = X_test[y_test_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQr_EdGkJM7E",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Training**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dQqMrihqmzU",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSdPtPG5tSwL",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LykNbu0WMyai",
    "outputId": "941a306e-c4c6-46c9-acc3-7285284e7aaa",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 61, 61, 64)        3136      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 29, 29, 64)        65600     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 29, 29, 64)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 26, 26, 128)       131200    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 12, 128)       262272    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 9, 9, 256)         524544    \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 3, 3, 256)         1048832   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 9)                 4617      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,220,361\n",
      "Trainable params: 3,220,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW7TOIJtqNgM",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-G8qzHZltlMX",
    "outputId": "3f87b637-7a2c-47c1-8937-4c805aa426d1",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "338/338 [==============================] - 311s 918ms/step - loss: 2.1246 - accuracy: 0.2140\n",
      "Epoch 2/5\n",
      "338/338 [==============================] - 284s 840ms/step - loss: 0.7039 - accuracy: 0.7558\n",
      "Epoch 3/5\n",
      "338/338 [==============================] - 290s 858ms/step - loss: 0.2715 - accuracy: 0.9205\n",
      "Epoch 4/5\n",
      "338/338 [==============================] - 291s 860ms/step - loss: 0.1474 - accuracy: 0.9568\n",
      "Epoch 5/5\n",
      "338/338 [==============================] - 265s 784ms/step - loss: 0.1035 - accuracy: 0.9732\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs = 5, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz9MgAjUJi8F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Đánh giá kết quả**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8imdaNQ0tnxD",
    "outputId": "9b1eb9e9-13af-47de-f231-7aafe05287a8",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test images: 96.481 %\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x = X_test, y = y_test, verbose = 0)\n",
    "print('Accuracy for test images:', round(score[1]*100, 3), '%')\n",
    "model.save(\"HandGesture.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d72b21489300652f2337cd7f80c57cc00ce6bbc92cf98d285c178d0a8ced433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
